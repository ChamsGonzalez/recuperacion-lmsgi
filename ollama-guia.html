<h1 id="ollama">Ollama</h1>
<p>Ollama es una plataforma de modelos de lenguaje grande (LLM) que te
permite ejecutar y gestionar modelos de lenguaje localmente. Se maneja a
través de la línea de comandos (CLI) pero puedes integrarlo con
diferentes interfaces gráficas y servidores web para facilitar su
uso.</p>
<figure>
<img src="enlace-foto-ollama.jpg" alt="imagen ollama" />
<figcaption aria-hidden="true">imagen ollama</figcaption>
</figure>
<h2 id="instalación">Instalación</h2>
<p>Para instalar Ollama, sigue estos pasos:</p>
<ol type="1">
<li>Abre la terminal en Linux.</li>
<li>Instala <code>curl</code> si no lo tienes instalado.</li>
<li>Ejecuta el siguiente comando, descargará un script y lo
ejecturará:</li>
</ol>
<div class="sourceCode" id="cb1"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-fsSL</span> https://ollama.com/install.sh <span class="kw">|</span> <span class="fu">sh</span></span></code></pre></div>
<ol start="4" type="1">
<li>Una vez instalado, puedes verificar la instalación ejecutando:</li>
</ol>
<div class="sourceCode" id="cb2"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> version</span></code></pre></div>
<h2 id="uso-básico">Uso básico</h2>
<p>Puedes descargar modelos preentrenados con
<code>ollama pull &lt;modelo&gt;</code> , por ejemplo:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> pull qwen2.5:0.5b</span></code></pre></div>
<p>Descargará el modelo <em>qwen2.5</em> de 0.5 billones de
parámetros.</p>
<p>Para listar los modelos disponibles localmente, usa:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> list</span></code></pre></div>
<p>Si quieres ejecutar un modelo y hacerle preguntas, usa:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run <span class="op">&lt;</span>modelo<span class="op">&gt;</span></span></code></pre></div>
<h2 id="acerca-de-los-modelos">Acerca de los modelos</h2>
<p>Los modelos tienen diferentes capacidades y tamaños. Siguiendo el
ejemplo anterior, en la <a
href="https://ollama.com/library/qwen2.5">página de Ollama</a> podemos
encontrar diferentes versiones de <em>qwen2.5</em></p>
<ul>
<li><code>qwen2.5:0.5b</code> - 0.5 billones de parámetros</li>
<li><code>qwen2.5:1.5b</code> - 1.5 billones de parámetros</li>
<li><code>qwen2.5:3b</code> - 3 billones de parámetros</li>
</ul>
<h2 id="usos-de-ollama">Usos de ollama</h2>
<p>Puedes usar Ollama para diversas tareas:</p>
<ul>
<li>Generación de texto</li>
<li>Resumen de documentos</li>
<li>Traducción de idiomas</li>
<li>Respuesta a preguntas</li>
<li>Integración con aplicaciones personalizadas</li>
</ul>
<p>El futuro es usar IA en local, no dependas de servicios en la nube, o
acabarás siendo dependiente de ellos, estarás regalando tus datos, tu
privacidad y tu libertad mientras les pagas por ello.</p>
